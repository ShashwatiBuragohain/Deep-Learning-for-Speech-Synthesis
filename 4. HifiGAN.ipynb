{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3211eab-75df-4eac-aabc-4fcd80b1396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5ad75-8f6b-4e07-8403-cbaa66a96249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b666cd7e-75be-45d8-9376-3a4f578b0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(r\"C:/Users/ADMIN/Downloads/SSP/VoiceProject/processed\")\n",
    "TRAIN_META = BASE_DIR / \"train_metadata.csv\"\n",
    "TEST_META  = BASE_DIR / \"test_metadata.csv\"\n",
    "NORM_MEL_DIR = BASE_DIR / \"normalized\"   # contains mel .npy\n",
    "CHECKPOINT_DIR = BASE_DIR / \"checkpoints/hifigan\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUNS_DIR = \"runs/hifigan\"\n",
    "\n",
    "SR = 16000\n",
    "N_MELS = 80\n",
    "HOP_LENGTH = 256\n",
    "WIN_LENGTH = 1024\n",
    "MEL_MEAN = np.load(BASE_DIR / \"mel_mean.npy\")\n",
    "MEL_STD  = np.load(BASE_DIR / \"mel_std.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98d104-f351-4ed8-9e0c-3165d8b727e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training hyperparams\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "EPOCHS = 100\n",
    "STEPS_PER_EPOCH = None   # set None to run through DataLoader fully\n",
    "LR_G = 2e-4\n",
    "LR_D = 2e-4\n",
    "BETAS = (0.9, 0.999)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = True\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Base dir:\", BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91b598e6-8aca-4ebf-83eb-fa7b87f9264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mel_file(utt_id: str):\n",
    "    \"\"\"Try common filename patterns in normalized dir.\"\"\"\n",
    "    candidates = [\n",
    "        NORM_MEL_DIR / f\"{utt_id}_mel.npy\",\n",
    "        NORM_MEL_DIR / f\"{utt_id}.npy\",\n",
    "        NORM_MEL_DIR / f\"{utt_id}_mel.npy\".replace(\"//\",\"/\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea7b51d-125d-48da-b70b-89cf2c98e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mel_for_vocoder(utt_id: str):\n",
    "    \"\"\"Load mel (denormalized) as float32 numpy array shape [T, n_mels].\"\"\"\n",
    "    p = find_mel_file(utt_id)\n",
    "    if p is None:\n",
    "        raise FileNotFoundError(f\"Mel not found for utt {utt_id}\")\n",
    "    mel = np.load(p).astype(np.float32)\n",
    "    # denormalize (HiFi-GAN expects mel in original log-power scale)\n",
    "    mel = mel * MEL_STD + MEL_MEAN\n",
    "    return mel  # [T, n_mels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fbd1138-6c48-4419-9727-ac6dee398b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(path: str, sr=SR):\n",
    "    \"\"\"Load waveform, convert to mono, resample to sr, return numpy float32 [-1,1].\"\"\"\n",
    "    wav, orig_sr = torchaudio.load(path)  # [channels, time]\n",
    "    if wav.ndim > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "    wav = wav.squeeze(0).numpy().astype(np.float32)\n",
    "    if orig_sr != sr:\n",
    "        wav = torchaudio.functional.resample(torch.from_numpy(wav), orig_sr, sr).numpy()\n",
    "    # ensure float32 and range [-1,1] (torchaudio does that by default)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cbc527e-70ca-49b9-9ada-af443f46a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class HiFiGANDataset(Dataset):\n",
    "    def __init__(self, metadata_csv, mel_dir, wav_mean=None, wav_std=None, sr=SR, max_wav_seconds=None):\n",
    "        self.df = pd.read_csv(metadata_csv)\n",
    "        self.mel_dir = Path(mel_dir)\n",
    "        self.sr = sr\n",
    "        # filter out missing mels/wavs\n",
    "        rows = []\n",
    "        for _, r in self.df.iterrows():\n",
    "            utt = str(r['utt_id'])\n",
    "            melp = find_mel_file(utt)\n",
    "            wavp = Path(r['path'])\n",
    "            if melp is None or not wavp.exists():\n",
    "                continue\n",
    "            rows.append(r)\n",
    "        self.df = pd.DataFrame(rows).reset_index(drop=True)\n",
    "        print(f\"HiFiGANDataset: {len(self.df)} examples (from {metadata_csv})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "        utt = str(r['utt_id'])\n",
    "        mel = load_mel_for_vocoder(utt)        # [T, n_mels]\n",
    "        wav = load_wav(r['path'], sr=self.sr)  # [time]\n",
    "        # convert types\n",
    "        mel = torch.from_numpy(mel.T).float() # [n_mels, T]  <-- HiFi-GAN uses [B, n_mels, T]\n",
    "        wav = torch.from_numpy(wav).float()   # [Twav]\n",
    "        return mel, wav, utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1648d4b0-3895-4a4c-955d-80eb12d3eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hifigan_collate(batch):\n",
    "    \"\"\"Pad mel and wav to longest in batch. Return tensors and lengths.\"\"\"\n",
    "    mels, wavs, utts = zip(*batch)\n",
    "    # mel lengths\n",
    "    mel_lens = [m.shape[1] for m in mels]\n",
    "    max_mel = max(mel_lens)\n",
    "    mel_pad = torch.zeros(len(mels), N_MELS, max_mel, dtype=torch.float32)\n",
    "    for i, m in enumerate(mels):\n",
    "        mel_pad[i, :, :m.shape[1]] = m\n",
    "\n",
    "    # waveform lengths (samples)\n",
    "    wav_lens = [w.shape[0] for w in wavs]\n",
    "    max_wav = max(wav_lens)\n",
    "    wav_pad = torch.zeros(len(wavs), max_wav, dtype=torch.float32)\n",
    "    for i, w in enumerate(wavs):\n",
    "        wav_pad[i, :w.shape[0]] = w\n",
    "\n",
    "    return mel_pad, torch.tensor(mel_lens), wav_pad, torch.tensor(wav_lens), list(utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46dac96b-7c8f-42f3-9a64-845deccf2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, dilation):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=((kernel_size-1)//2)*dilation, dilation=dilation)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.act = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        return self.act(out + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8251f1c-f077-46e8-9b4c-6129646234f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mel_channels=80,\n",
    "        upsample_rates=(8, 8, 2, 2),\n",
    "        upsample_kernel_sizes=(16, 16, 4, 4),\n",
    "        resblock_kernel_sizes=(3, 3, 3),\n",
    "        resblock_dilations=((1, 3, 5), (1, 3, 5), (1, 3, 5))\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # initial projection\n",
    "        self.conv_in = nn.Conv1d(mel_channels, 512, kernel_size=7, padding=3)\n",
    "\n",
    "        # upsampling layers\n",
    "        self.ups = nn.ModuleList()\n",
    "        in_ch = 512\n",
    "        for r, k in zip(upsample_rates, upsample_kernel_sizes):\n",
    "            out_ch = in_ch // 2\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose1d(\n",
    "                    in_ch,\n",
    "                    out_ch,\n",
    "                    kernel_size=k,\n",
    "                    stride=r,\n",
    "                    padding=(k - r) // 2\n",
    "                )\n",
    "            )\n",
    "            in_ch = out_ch  # update for next stage\n",
    "\n",
    "        # residual blocks (1 per upsample stage for simplicity)\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        in_ch = 512\n",
    "        for i in range(len(self.ups)):\n",
    "            out_ch = in_ch // 2\n",
    "            for k, d_tuple in zip(resblock_kernel_sizes, resblock_dilations):\n",
    "                for d in d_tuple:\n",
    "                    self.resblocks.append(ResBlock(out_ch, k, d))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        # final conv\n",
    "        self.conv_out = nn.Conv1d(in_ch, 1, kernel_size=7, padding=3)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, m):\n",
    "        # m: [B, n_mels, Tm]\n",
    "        x = self.conv_in(m)\n",
    "        cur = x\n",
    "        for i, up in enumerate(self.ups):\n",
    "            cur = up(cur)\n",
    "            cur = F.relu(cur)\n",
    "            # (you can apply resblocks here if you want more realism)\n",
    "        out = self.conv_out(cur)\n",
    "        return self.tanh(out).squeeze(1)  # [B, Ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "901d7e28-d7b7-4c7b-9ad0-ceb280b2cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple multi-scale discriminator\n",
    "class DiscriminatorP(nn.Module):\n",
    "    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n",
    "        super().__init__()\n",
    "        self.period = period\n",
    "        def SNConv(in_ch, out_ch, k, s, p):\n",
    "            c = nn.Conv1d(in_ch, out_ch, k, s, p)\n",
    "            return c\n",
    "        self.convs = nn.ModuleList([\n",
    "            SNConv(1, 32, 5, 3, 2),\n",
    "            SNConv(32, 128, 5, 3, 2),\n",
    "            SNConv(128, 512, 5, 3, 2),\n",
    "            SNConv(512, 1024, 5, 3, 2),\n",
    "            SNConv(1024, 1024, 5, 1, 2),\n",
    "        ])\n",
    "        self.final = nn.Conv1d(1024, 1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T]\n",
    "        b, t = x.shape\n",
    "        if t % self.period != 0:\n",
    "            pad_len = self.period - (t % self.period)\n",
    "            x = F.pad(x, (0, pad_len))\n",
    "            t = t + pad_len\n",
    "        x = x.view(b, 1, t)\n",
    "        features = []\n",
    "        for c in self.convs:\n",
    "            x = F.leaky_relu(c(x), 0.1)\n",
    "            features.append(x)\n",
    "        out = self.final(x)\n",
    "        features.append(out)\n",
    "        return out, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e29d9e-43c5-48b3-bd2d-d0699dd849ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleDiscriminator(nn.Module):\n",
    "    def __init__(self, periods=(2,3,5)):\n",
    "        super().__init__()\n",
    "        self.discriminators = nn.ModuleList([DiscriminatorP(p) for p in periods])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        feats = []\n",
    "        for d in self.discriminators:\n",
    "            out, f = d(x)\n",
    "            outs.append(out)\n",
    "            feats.append(f)\n",
    "        return outs, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cf56149-9b6b-4dfa-aa9d-20244a7880cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses\n",
    "adv_criterion = nn.MSELoss()  # LSGAN style\n",
    "mel_criterion = nn.L1Loss()\n",
    "\n",
    "def discriminator_loss(real_scores, fake_scores):\n",
    "    loss = 0.0\n",
    "    for r, f in zip(real_scores, fake_scores):\n",
    "        loss += adv_criterion(r, torch.ones_like(r)) + adv_criterion(f, torch.zeros_like(f))\n",
    "    return loss\n",
    "\n",
    "def generator_adversarial_loss(fake_scores):\n",
    "    loss = 0.0\n",
    "    for f in fake_scores:\n",
    "        loss += adv_criterion(f, torch.ones_like(f))\n",
    "    return loss\n",
    "\n",
    "def feature_matching_loss(real_feats, fake_feats):\n",
    "    loss = 0.0\n",
    "    for rf_list, ff_list in zip(real_feats, fake_feats):\n",
    "        for r_feat, f_feat in zip(rf_list, ff_list):\n",
    "            loss += F.l1_loss(f_feat, r_feat.detach())\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0a7aac7-6dff-40e7-b523-e90d6d8717fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(d_fake_mpd, d_fake_msd, fmap_r_mpd, fmap_g_mpd, fmap_r_msd, fmap_g_msd, fake_wav, real_wav):\n",
    "    \"\"\"\n",
    "    Complete generator loss for HiFi-GAN.\n",
    "    Handles variable length feature maps by truncating to the minimum length.\n",
    "    \"\"\"\n",
    "    # Adversarial loss\n",
    "    adv_loss = 0.0\n",
    "    for f_mpd, f_msd in zip(d_fake_mpd, d_fake_msd):\n",
    "        adv_loss += adv_criterion(f_mpd, torch.ones_like(f_mpd))\n",
    "        adv_loss += adv_criterion(f_msd, torch.ones_like(f_msd))\n",
    "    \n",
    "    # Feature matching loss - handle variable lengths\n",
    "    fm_loss = 0.0\n",
    "    \n",
    "    # For Multi-Period Discriminator features\n",
    "    for rf_list, gf_list in zip(fmap_r_mpd, fmap_g_mpd):\n",
    "        for r_feat, g_feat in zip(rf_list, gf_list):\n",
    "            # Truncate to minimum length\n",
    "            min_length = min(r_feat.size(-1), g_feat.size(-1))\n",
    "            r_feat_trunc = r_feat[..., :min_length]\n",
    "            g_feat_trunc = g_feat[..., :min_length]\n",
    "            fm_loss += F.l1_loss(g_feat_trunc, r_feat_trunc.detach())\n",
    "    \n",
    "    # For Multi-Scale Discriminator features\n",
    "    for rf_list, gf_list in zip(fmap_r_msd, fmap_g_msd):\n",
    "        for r_feat, g_feat in zip(rf_list, gf_list):\n",
    "            # Truncate to minimum length\n",
    "            min_length = min(r_feat.size(-1), g_feat.size(-1))\n",
    "            r_feat_trunc = r_feat[..., :min_length]\n",
    "            g_feat_trunc = g_feat[..., :min_length]\n",
    "            fm_loss += F.l1_loss(g_feat_trunc, r_feat_trunc.detach())\n",
    "    \n",
    "    # Mel-spectrogram reconstruction loss\n",
    "    mel_fake = mel_spectrogram(fake_wav)\n",
    "    mel_real = mel_spectrogram(real_wav)\n",
    "    \n",
    "    # Match lengths\n",
    "    min_len = min(mel_fake.size(-1), mel_real.size(-1))\n",
    "    mel_fake = mel_fake[..., :min_len]\n",
    "    mel_real = mel_real[..., :min_len]\n",
    "    \n",
    "    mel_loss = mel_criterion(mel_fake, mel_real)\n",
    "    \n",
    "    # Weighting factors (standard HiFi-GAN weights)\n",
    "    lambda_adv = 1.0\n",
    "    lambda_fm = 2.0\n",
    "    lambda_mel = 45.0\n",
    "    \n",
    "    total_loss = (lambda_adv * adv_loss + \n",
    "                 lambda_fm * fm_loss + \n",
    "                 lambda_mel * mel_loss)\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afeb609a-0131-4686-a215-0bce88ba82cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen params (M): 7.659137\n",
      "Disc params (M): 24.655299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_15580\\3636663923.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n"
     ]
    }
   ],
   "source": [
    "# init models + optimizers\n",
    "gen = Generator().to(DEVICE)\n",
    "disc = MultiScaleDiscriminator().to(DEVICE)\n",
    "\n",
    "opt_g = torch.optim.AdamW(gen.parameters(), lr=LR_G, betas=BETAS)\n",
    "opt_d = torch.optim.AdamW(disc.parameters(), lr=LR_D, betas=BETAS)\n",
    "# learning rate schedulers \n",
    "scheduler_g = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    opt_g, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "scheduler_d = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    opt_d, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "print(\"Gen params (M):\", sum(p.numel() for p in gen.parameters())/1e6)\n",
    "print(\"Disc params (M):\", sum(p.numel() for p in disc.parameters())/1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eca4da3f-759b-4bdb-86d2-114ef0548749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HiFiGANDataset: 29102 examples (from C:\\Users\\ADMIN\\Downloads\\SSP\\VoiceProject\\processed\\train_metadata.csv)\n",
      "HiFiGANDataset: 4134 examples (from C:\\Users\\ADMIN\\Downloads\\SSP\\VoiceProject\\processed\\test_metadata.csv)\n"
     ]
    }
   ],
   "source": [
    "# dataloaders\n",
    "train_ds = HiFiGANDataset(TRAIN_META, NORM_MEL_DIR)\n",
    "val_ds   = HiFiGANDataset(TEST_META, NORM_MEL_DIR)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=hifigan_collate)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=hifigan_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0aebab5-f70e-4e69-9823-e835eba05c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodDiscriminator(nn.Module):\n",
    "    def __init__(self, period=2):\n",
    "        super().__init__()\n",
    "        self.period = period\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, 32, (5, 1), (3, 1), padding=(2, 0)),\n",
    "            nn.Conv2d(32, 128, (5, 1), (3, 1), padding=(2, 0)),\n",
    "            nn.Conv2d(128, 512, (5, 1), (3, 1), padding=(2, 0))\n",
    "        ])\n",
    "        self.conv_post = nn.Conv2d(512, 1, (3, 1), 1, padding=(1, 0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "        # reshape waveform into [B, 1, T//p, p]\n",
    "        b, t = x.size(0), x.size(1)\n",
    "        if t % self.period != 0:\n",
    "            pad_len = self.period - (t % self.period)\n",
    "            x = F.pad(x, (0, pad_len), \"reflect\")\n",
    "        x = x.view(b, 1, -1, self.period)\n",
    "\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            fmap.append(x)\n",
    "\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        return torch.flatten(x, 1, -1), fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddee67a7-70c3-4d90-b9ad-90ec3f057710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPeriodDiscriminator(nn.Module):\n",
    "    def __init__(self, periods=[2,3,5,7,11]):\n",
    "        super().__init__()\n",
    "        self.discriminators = nn.ModuleList([PeriodDiscriminator(p) for p in periods])\n",
    "\n",
    "    def forward(self, real, fake):\n",
    "        real_outputs, fake_outputs, real_feats, fake_feats = [], [], [], []\n",
    "        for d in self.discriminators:\n",
    "            r_out, r_feat = d(real)\n",
    "            f_out, f_feat = d(fake)\n",
    "            real_outputs.append(r_out)\n",
    "            fake_outputs.append(f_out)\n",
    "            real_feats.append(r_feat)\n",
    "            fake_feats.append(f_feat)\n",
    "        return real_outputs, fake_outputs, real_feats, fake_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51f080fb-9815-4db2-b8db-4e79361b3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(1, 128, 15, 1, padding=7),\n",
    "            nn.Conv1d(128, 128, 41, 4, groups=4, padding=20),\n",
    "            nn.Conv1d(128, 256, 41, 4, groups=16, padding=20),\n",
    "            nn.Conv1d(256, 512, 41, 4, groups=16, padding=20),\n",
    "            nn.Conv1d(512, 1024, 41, 4, groups=16, padding=20),\n",
    "            nn.Conv1d(1024, 1024, 5, 1, padding=2)\n",
    "        ])\n",
    "        self.conv_post = nn.Conv1d(1024, 1, 3, 1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        return torch.flatten(x, 1, -1), fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59231dde-0975-4dec-a560-e9627797cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.discriminators = nn.ModuleList([ScaleDiscriminator() for _ in range(3)])\n",
    "        self.pools = nn.ModuleList([nn.Identity(), nn.AvgPool1d(4, 2, padding=2), nn.AvgPool1d(4, 2, padding=2)])\n",
    "\n",
    "    def forward(self, real, fake):\n",
    "        real_outputs, fake_outputs, real_feats, fake_feats = [], [], [], []\n",
    "        for pool, d in zip(self.pools, self.discriminators):\n",
    "            r_out, r_feat = d(pool(real.unsqueeze(1)))\n",
    "            f_out, f_feat = d(pool(fake.unsqueeze(1)))\n",
    "            real_outputs.append(r_out)\n",
    "            fake_outputs.append(f_out)\n",
    "            real_feats.append(r_feat)\n",
    "            fake_feats.append(f_feat)\n",
    "        return real_outputs, fake_outputs, real_feats, fake_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6c3bc6c-687a-4633-b519-48220587a8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_15580\\927379973.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "gen = Generator().to(DEVICE)\n",
    "mpd = MultiPeriodDiscriminator().to(DEVICE)\n",
    "msd = MultiScaleDiscriminator().to(DEVICE)\n",
    "# Optimizers\n",
    "opt_g = torch.optim.AdamW(gen.parameters(), lr=2e-4, betas=(0.8, 0.99))\n",
    "opt_d = torch.optim.AdamW(list(mpd.parameters()) + list(msd.parameters()), lr=2e-4, betas=(0.8, 0.99))\n",
    "\n",
    "# Mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b6390cc-3985-4c97-9c01-c77ff3ff5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, gen, mpd, msd, train_loader,\n",
    "                    optimizer_g, optimizer_d, scaler, DEVICE, USE_AMP=True):\n",
    "    \"\"\"\n",
    "    Train HiFi-GAN for one epoch (memory-optimized).\n",
    "    \"\"\"\n",
    "    gen.train()\n",
    "    mpd.train()\n",
    "    msd.train()\n",
    "\n",
    "    running_g, running_d = 0.0, 0.0\n",
    "    print(f\"\\n[Epoch {epoch}] Training...\")\n",
    "    print_cuda_memory(\"Start of Epoch\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Unpack all 5 values from the collate function\n",
    "        mel_pad, mel_len, wav_pad, wav_len, utts = batch\n",
    "\n",
    "        mel_pad = mel_pad.to(DEVICE, non_blocking=True)\n",
    "        wav_pad = wav_pad.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        if isinstance(mel_len, torch.Tensor):\n",
    "            mel_len = mel_len.to(DEVICE, non_blocking=True)\n",
    "        if isinstance(wav_len, torch.Tensor):\n",
    "            wav_len = wav_len.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer_g.zero_grad(set_to_none=True)\n",
    "        optimizer_d.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Train Discriminator\n",
    "        with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "            fake_wav = gen(mel_pad)\n",
    "            real_wav = wav_pad\n",
    "\n",
    "            # Ensure lengths match by truncating\n",
    "            min_length = min(fake_wav.size(-1), real_wav.size(-1))\n",
    "            fake_wav = fake_wav[..., :min_length]\n",
    "            real_wav = real_wav[..., :min_length]\n",
    "\n",
    "            d_real_mpd, d_fake_mpd, _, _ = mpd(real_wav, fake_wav.detach())\n",
    "            d_real_msd, d_fake_msd, _, _ = msd(real_wav, fake_wav.detach())\n",
    "            d_loss = (discriminator_loss(d_real_mpd, d_fake_mpd) +\n",
    "                      discriminator_loss(d_real_msd, d_fake_msd))\n",
    "\n",
    "        # Scale and backward for discriminator\n",
    "        scaler.scale(d_loss).backward()\n",
    "        scaler.step(optimizer_d)  # Update discriminator weights\n",
    "\n",
    "        # Train Generator\n",
    "        with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "            d_real_mpd, d_fake_mpd, fmap_r_mpd, fmap_g_mpd = mpd(real_wav, fake_wav)\n",
    "            d_real_msd, d_fake_msd, fmap_r_msd, fmap_g_msd = msd(real_wav, fake_wav)\n",
    "\n",
    "            g_loss = generator_loss(\n",
    "                d_fake_mpd, d_fake_msd,\n",
    "                fmap_r_mpd, fmap_g_mpd,\n",
    "                fmap_r_msd, fmap_g_msd,\n",
    "                fake_wav, real_wav\n",
    "            )\n",
    "\n",
    "        # Scale and backward for generator\n",
    "        scaler.scale(g_loss).backward()\n",
    "        scaler.step(optimizer_g)  # Update generator weights\n",
    "        \n",
    "        # Update the scaler only once per batch\n",
    "        scaler.update()\n",
    "\n",
    "        #   Logging + Cleanup\n",
    "        running_d += d_loss.item()\n",
    "        running_g += g_loss.item()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            avg_g = running_g / (batch_idx + 1)\n",
    "            avg_d = running_d / (batch_idx + 1)\n",
    "            print(f\"  Batch {batch_idx:04d} | g_loss={avg_g:.4f}, d_loss={avg_d:.4f}\")\n",
    "            print_cuda_memory(f\"After batch {batch_idx}\")\n",
    "\n",
    "        # Free unused references (light cleanup)\n",
    "        del fake_wav, real_wav\n",
    "\n",
    "    print_cuda_memory(\"End of Epoch\")\n",
    "\n",
    "    return {\n",
    "        \"g_loss\": running_g / len(train_loader),\n",
    "        \"d_loss\": running_d / len(train_loader),\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(epoch, gen, val_loader, DEVICE, USE_AMP=True):\n",
    "    \"\"\"\n",
    "    Validate HiFi-GAN for one epoch (memory-optimized).\n",
    "    Only computes mel-spectrogram reconstruction loss (L1).\n",
    "    \"\"\"\n",
    "    gen.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    print(f\"\\n[Epoch {epoch}] Validation...\")\n",
    "    print_cuda_memory(\"Start of Validation\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(val_loader):\n",
    "        # Unpack all 5 values from the collate function\n",
    "        mel_pad, mel_len, wav_pad, wav_len, utts = batch  # ← Added utts here\n",
    "        mel_pad = mel_pad.to(DEVICE, non_blocking=True)\n",
    "        wav_pad = wav_pad.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "            fake_wav = gen(mel_pad)\n",
    "\n",
    "            # Match lengths\n",
    "            min_len = min(fake_wav.size(-1), wav_pad.size(-1))\n",
    "            fake_wav = fake_wav[..., :min_len]\n",
    "            real_wav = wav_pad[..., :min_len]\n",
    "\n",
    "            # Compute mel loss\n",
    "            mel_fake = mel_spectrogram(fake_wav)\n",
    "            mel_real = mel_spectrogram(real_wav)\n",
    "            loss = F.l1_loss(mel_fake, mel_real)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            avg_loss = val_loss / (batch_idx + 1)\n",
    "            print(f\"  Val batch {batch_idx:04d} | mel_loss={avg_loss:.4f}\")\n",
    "            print_cuda_memory(f\"After val batch {batch_idx}\")\n",
    "\n",
    "        # cleanup\n",
    "        del fake_wav, real_wav, mel_fake, mel_real\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"[Epoch {epoch}] Validation Loss: {val_loss:.4f}\")\n",
    "    print_cuda_memory(\"End of Validation\")\n",
    "\n",
    "    return {\"val_loss\": val_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daf75ca6-9982-4ff7-995d-438bae198e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(epoch, gen, mpd, msd, train_loader,\n",
    "                    optimizer_g, optimizer_d, scaler, DEVICE, USE_AMP=True):\n",
    "    \"\"\"\n",
    "    Train HiFi-GAN for one epoch with progress bar.\n",
    "    \"\"\"\n",
    "    gen.train()\n",
    "    mpd.train()\n",
    "    msd.train()\n",
    "\n",
    "    running_g, running_d = 0.0, 0.0\n",
    "    print(f\"\\n[Epoch {epoch}] Training...\")\n",
    "    print_cuda_memory(\"Start of Epoch\")\n",
    "\n",
    "    # Add progress bar\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} Training\", leave=False)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Unpack all 5 values from the collate function\n",
    "        mel_pad, mel_len, wav_pad, wav_len, utts = batch\n",
    "\n",
    "        mel_pad = mel_pad.to(DEVICE, non_blocking=True)\n",
    "        wav_pad = wav_pad.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        if isinstance(mel_len, torch.Tensor):\n",
    "            mel_len = mel_len.to(DEVICE, non_blocking=True)\n",
    "        if isinstance(wav_len, torch.Tensor):\n",
    "            wav_len = wav_len.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer_g.zero_grad(set_to_none=True)\n",
    "        optimizer_d.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Train Discriminator\n",
    "        with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "            fake_wav = gen(mel_pad)\n",
    "            real_wav = wav_pad\n",
    "\n",
    "            # Ensure lengths match by truncating\n",
    "            min_length = min(fake_wav.size(-1), real_wav.size(-1))\n",
    "            fake_wav = fake_wav[..., :min_length]\n",
    "            real_wav = real_wav[..., :min_length]\n",
    "\n",
    "            d_real_mpd, d_fake_mpd, _, _ = mpd(real_wav, fake_wav.detach())\n",
    "            d_real_msd, d_fake_msd, _, _ = msd(real_wav, fake_wav.detach())\n",
    "            d_loss = (discriminator_loss(d_real_mpd, d_fake_mpd) +\n",
    "                      discriminator_loss(d_real_msd, d_fake_msd))\n",
    "\n",
    "        # Scale and backward for discriminator\n",
    "        scaler.scale(d_loss).backward()\n",
    "        scaler.step(optimizer_d)\n",
    "\n",
    "        # Train Generator\n",
    "        with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "            d_real_mpd, d_fake_mpd, fmap_r_mpd, fmap_g_mpd = mpd(real_wav, fake_wav)\n",
    "            d_real_msd, d_fake_msd, fmap_r_msd, fmap_g_msd = msd(real_wav, fake_wav)\n",
    "\n",
    "            g_loss = generator_loss(\n",
    "                d_fake_mpd, d_fake_msd,\n",
    "                fmap_r_mpd, fmap_g_mpd,\n",
    "                fmap_r_msd, fmap_g_msd,\n",
    "                fake_wav, real_wav\n",
    "            )\n",
    "\n",
    "        # Scale and backward for generator\n",
    "        scaler.scale(g_loss).backward()\n",
    "        scaler.step(optimizer_g)\n",
    "        scaler.update()\n",
    "\n",
    "        # Update running losses\n",
    "        running_d += d_loss.item()\n",
    "        running_g += g_loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        avg_g = running_g / (batch_idx + 1)\n",
    "        avg_d = running_d / (batch_idx + 1)\n",
    "        pbar.set_postfix({\n",
    "            'g_loss': f'{avg_g:.4f}',\n",
    "            'd_loss': f'{avg_d:.4f}',\n",
    "            'gpu_mem': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB'\n",
    "        })\n",
    "\n",
    "        # Free unused references\n",
    "        del fake_wav, real_wav\n",
    "\n",
    "    pbar.close()\n",
    "    print_cuda_memory(\"End of Epoch\")\n",
    "\n",
    "    return {\n",
    "        \"g_loss\": running_g / len(train_loader),\n",
    "        \"d_loss\": running_d / len(train_loader),\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(epoch, gen, val_loader, DEVICE, USE_AMP=True):\n",
    "    \"\"\"\n",
    "    Validate HiFi-GAN for one epoch with progress bar.\n",
    "    \"\"\"\n",
    "    gen.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    print(f\"\\n[Epoch {epoch}] Validation...\")\n",
    "    print_cuda_memory(\"Start of Validation\")\n",
    "\n",
    "    # Add progress bar for validation\n",
    "    pbar = tqdm(val_loader, desc=f\"Epoch {epoch} Validation\", leave=False)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Unpack all 5 values from the collate function\n",
    "        mel_pad, mel_len, wav_pad, wav_len, utts = batch\n",
    "        mel_pad = mel_pad.to(DEVICE, non_blocking=True)\n",
    "        wav_pad = wav_pad.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "            fake_wav = gen(mel_pad)\n",
    "\n",
    "            # Match lengths\n",
    "            min_len = min(fake_wav.size(-1), wav_pad.size(-1))\n",
    "            fake_wav = fake_wav[..., :min_len]\n",
    "            real_wav = wav_pad[..., :min_len]\n",
    "\n",
    "            # Compute mel loss\n",
    "            mel_fake = mel_spectrogram(fake_wav)\n",
    "            mel_real = mel_spectrogram(real_wav)\n",
    "            loss = F.l1_loss(mel_fake, mel_real)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        avg_loss = val_loss / (batch_idx + 1)\n",
    "        pbar.set_postfix({\n",
    "            'val_loss': f'{avg_loss:.4f}',\n",
    "            'gpu_mem': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB'\n",
    "        })\n",
    "\n",
    "        # cleanup\n",
    "        del fake_wav, real_wav, mel_fake, mel_real\n",
    "\n",
    "    pbar.close()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"[Epoch {epoch}] Validation Loss: {val_loss:.4f}\")\n",
    "    print_cuda_memory(\"End of Validation\")\n",
    "\n",
    "    return {\"val_loss\": val_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32ba0ead-4456-4f50-9863-c9430320aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_15580\\4016092911.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n"
     ]
    }
   ],
   "source": [
    "# Optimizers and Scaler \n",
    "LEARNING_RATE = 2e-4  # default for HiFi-GAN\n",
    "betas = (0.8, 0.99)\n",
    "\n",
    "optimizer_g = torch.optim.AdamW(gen.parameters(), lr=LEARNING_RATE, betas=betas)\n",
    "optimizer_d = torch.optim.AdamW(\n",
    "    list(mpd.parameters()) + list(msd.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    "    betas=betas\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "309a25cd-21b4-4a1a-bca0-15595c0a96f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_spectrogram(wav):\n",
    "    \"\"\"Convert waveform to mel-spectrogram using the same parameters as preprocessing.\"\"\"\n",
    "    # You'll need to implement this using torchaudio or your preferred method\n",
    "    # This should match the parameters used during data preprocessing\n",
    "    # (SR=16000, N_MELS=80, HOP_LENGTH=256, WIN_LENGTH=1024)\n",
    "    \n",
    "    # Example implementation using torchaudio:\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SR,\n",
    "        n_fft=WIN_LENGTH,\n",
    "        win_length=WIN_LENGTH,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        n_mels=N_MELS,\n",
    "        center=True\n",
    "    ).to(wav.device)\n",
    "    \n",
    "    mel = mel_transform(wav)\n",
    "    mel = torch.log(torch.clamp(mel, min=1e-5))  # Convert to log scale\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5abcb1-f5a1-4022-8f05-a9c581a6ba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded checkpoint from checkpoints\\checkpoint_last.pt, resuming at epoch 2\n",
      "\n",
      "[Epoch 2] Training...\n",
      "[Start of Epoch] CUDA Memory - Allocated: 444.89 MB | Reserved: 580.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training:   0%|              | 4/7276 [00:03<1:16:17,  1.59it/s, g_loss=133.3203, d_loss=2.7370, gpu_mem=4.8GB]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Directory to save checkpoints\n",
    "CHECKPOINT_DIR = Path(\"./checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Checkpoint Helpers\n",
    "def save_checkpoint(epoch, gen, mpd, msd, optimizer_g, optimizer_d, scaler, path):\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"gen\": gen.state_dict(),\n",
    "        \"mpd\": mpd.state_dict(),\n",
    "        \"msd\": msd.state_dict(),\n",
    "        \"optimizer_g\": optimizer_g.state_dict(),\n",
    "        \"optimizer_d\": optimizer_d.state_dict(),\n",
    "        \"scaler\": scaler.state_dict()\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f\" Checkpoint saved at {path}\")\n",
    "\n",
    "def load_checkpoint(path, gen, mpd, msd, optimizer_g, optimizer_d, scaler, device):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    gen.load_state_dict(checkpoint[\"gen\"])\n",
    "    mpd.load_state_dict(checkpoint[\"mpd\"])\n",
    "    msd.load_state_dict(checkpoint[\"msd\"])\n",
    "    optimizer_g.load_state_dict(checkpoint[\"optimizer_g\"])\n",
    "    optimizer_d.load_state_dict(checkpoint[\"optimizer_d\"])\n",
    "    scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    print(f\" Loaded checkpoint from {path}, resuming at epoch {start_epoch}\")\n",
    "    return start_epoch\n",
    "\n",
    "def print_cuda_memory(tag=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "        print(f\"[{tag}] CUDA Memory - Allocated: {allocated:.2f} MB | Reserved: {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"[{tag}] CUDA not available\")\n",
    "\n",
    "#  Training Loop\n",
    "start_epoch = 1\n",
    "resume_ckpt = CHECKPOINT_DIR / \"checkpoint_last.pt\"  # change if needed\n",
    "\n",
    "# Resume if checkpoint exists\n",
    "if resume_ckpt.exists():\n",
    "    start_epoch = load_checkpoint(resume_ckpt, gen, mpd, msd, optimizer_g, optimizer_d, scaler, DEVICE)\n",
    "# Early stopping parameters\n",
    "PATIENCE = 10\n",
    "MIN_DELTA = 0.001\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "early_stop = False\n",
    "\n",
    "# Training Loop with progress bars and schedulers\n",
    "for epoch in range(start_epoch, EPOCHS+1):\n",
    "    if early_stop:\n",
    "        print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "        \n",
    "    t0 = time.time()\n",
    "\n",
    "    # Training with progress bar\n",
    "    train_stats = train_one_epoch(\n",
    "        epoch,\n",
    "        gen,\n",
    "        mpd,\n",
    "        msd,\n",
    "        train_loader,\n",
    "        optimizer_g,\n",
    "        optimizer_d,\n",
    "        scaler,\n",
    "        DEVICE,\n",
    "        USE_AMP\n",
    "    )\n",
    "\n",
    "    # Validation with progress bar\n",
    "    val_stats = validate_epoch(\n",
    "        epoch,\n",
    "        gen,\n",
    "        val_loader,\n",
    "        DEVICE,\n",
    "        USE_AMP\n",
    "    )\n",
    "    \n",
    "    current_val_loss = val_stats[\"val_loss\"]\n",
    "    \n",
    "    # Update learning rate schedulers\n",
    "    scheduler_g.step(current_val_loss)\n",
    "    scheduler_d.step(current_val_loss)\n",
    "    \n",
    "    dt = time.time() - t0\n",
    "    \n",
    "    print(f\" Epoch {epoch} finished in {dt/60:.2f} min | \"\n",
    "          f\"G Loss: {train_stats['g_loss']:.4f} | D Loss: {train_stats['d_loss']:.4f} | \"\n",
    "          f\"Val Loss: {current_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if current_val_loss < best_val_loss - MIN_DELTA:\n",
    "        best_val_loss = current_val_loss\n",
    "        patience_counter = 0\n",
    "        best_ckpt_path = CHECKPOINT_DIR / \"checkpoint_best.pt\"\n",
    "        save_checkpoint(epoch, gen, mpd, msd, optimizer_g, optimizer_d, scaler, best_ckpt_path)\n",
    "        print(f\"✓ New best validation loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"✗ Validation loss didn't improve. Patience: {patience_counter}/{PATIENCE}\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            early_stop = True\n",
    "            print(\"Early stopping triggered!\")\n",
    "\n",
    "    # Save checkpoints\n",
    "    ckpt_path = CHECKPOINT_DIR / f\"checkpoint_epoch{epoch}.pt\"\n",
    "    save_checkpoint(epoch, gen, mpd, msd, optimizer_g, optimizer_d, scaler, ckpt_path)\n",
    "    save_checkpoint(epoch, gen, mpd, msd, optimizer_g, optimizer_d, scaler, resume_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc5140e-dcdf-4515-8537-b8253d95c234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-gpu)",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
